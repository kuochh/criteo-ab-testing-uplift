{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c657dd53-179f-4491-87e8-a66a6031de31",
   "metadata": {},
   "source": [
    " # Appendix: Meta-Learner Implementations - Algorithm Understanding Through Hand-Coded Models\n",
    "\n",
    " Technical deep dive demonstrating algorithmic understanding of uplift modeling meta-learners. Each approach estimates Conditional Average Treatment Effects (CATE) using different methodologies, implemented from scratch and validated against EconML library\n",
    "\n",
    " **CATE Definition:**\n",
    " $$CATE(x) = \\tau(x) = E[Y(1)−Y(0)∣X=x] = E[Y∣T=1, X=x]−E[Y∣T=0,X=x]$$\n",
    "\n",
    " **Implementation Process:**\n",
    " 1. **Data Setup**: Identical to Notebooks, 60/20/20 train/validation/test split with 10% of the sample\n",
    " 2. **Model Training**: Implement each meta-learner approach using XGBoost base models with fixed hyperparameters\n",
    " 3. **Final Training**: Train on combined train+validation data\n",
    " 4. **Evaluation**: Test on held-out test set\n",
    " 5. **Verification**: Compare hand-coded implementations with EconML library\n",
    "\n",
    " **Purpose**: Demonstrates understanding beyond API calls while validating production-quality implementations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1443c8-5653-422f-a672-b87124cd1928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklift.metrics import uplift_auc_score\n",
    "from econml.metalearners import SLearner, TLearner, XLearner\n",
    "from econml.dr import DRLearner\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Wrapper to make classifier output probabilities for EconML\n",
    "# EconML expects continuous outputs for regression tasks, but we use classifiers\n",
    "# for binary outcomes. This wrapper makes classifiers compatible by outputting\n",
    "# probabilities instead of discrete class predictions.\n",
    "class ClassifierAsRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Wrapper to make a classifier behave like a regressor by outputting probabilities\"\"\"\n",
    "    def __init__(self, classifier):\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classifier.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Return probability of positive class instead of discrete prediction\n",
    "        return self.classifier.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f5fcd-546a-465e-a28a-2130383ea23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPARATION\n",
    "# Load 10% sample of Criteo dataset for faster execution\n",
    "df = pl.read_csv(\"data/criteo-uplift-v2.1.csv\").sample(fraction=0.10, seed=42)\n",
    "\n",
    "# Data preparation\n",
    "feature_cols = [f'f{i}' for i in range(12)]\n",
    "X = df.select(feature_cols).to_numpy()\n",
    "T = df.select('treatment').to_numpy().ravel()\n",
    "Y = df.select('conversion').to_numpy().ravel()\n",
    "\n",
    "# Train/validation/test split (60/20/20)\n",
    "X_train, X_temp, Y_train, Y_temp, T_train, T_temp = train_test_split(\n",
    "    X, Y, T, test_size=0.4, random_state=42, stratify=T\n",
    ")\n",
    "X_val, X_test, Y_val, Y_test, T_val, T_test = train_test_split(\n",
    "    X_temp, Y_temp, T_temp, test_size=0.5, random_state=42, stratify=T_temp\n",
    ")\n",
    "\n",
    "# Combine train+val for final training\n",
    "X_trainval = np.vstack([X_train, X_val])\n",
    "Y_trainval = np.hstack([Y_train, Y_val])\n",
    "T_trainval = np.hstack([T_train, T_val])\n",
    "\n",
    "# Final data shapes: Train+Val (80%) for model training, Test (20%) for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f16d8a-cad2-4722-868a-3a1ccba5d790",
   "metadata": {},
   "source": [
    " ## S-Learner (Single Model Approach)\n",
    "\n",
    " **Method:** Train one model with treatment as a feature to estimate E[Y|X,T], then predict outcomes under both treatment conditions\n",
    "\n",
    " **Algorithm:**\n",
    " 1. **Model Training**: Single model learns E[Y|X,T] using features X plus treatment indicator T\n",
    " 2. **CATE Estimation**: τ(X) = μ(X,T=1) - μ(X,T=0) by predicting for each observation under both treatments\n",
    " 3. **Advantages**: Simple, uses all data for one model\n",
    " 4. **Disadvantages**: Assumes treatment effect is linear in model specification\n",
    "\n",
    " **Implementation**: XGBoost classifier with treatment as additional feature, then difference predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346534b3-bb5b-40b5-a8fb-eb9abd388887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a fix set of hyperparameters - skipping grid search\n",
    "best_s_params = {'learning_rate': 0.01, 'scale_pos_weight': 50}\n",
    "\n",
    "# Create augmented features\n",
    "X_trainval_with_treatment = np.column_stack([X_trainval, T_trainval])\n",
    "X_test_treatment = np.column_stack([X_test, np.ones(len(X_test))])\n",
    "X_test_control = np.column_stack([X_test, np.zeros(len(X_test))])\n",
    "\n",
    "# Train S-Learner model\n",
    "s_model = XGBClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=best_s_params['learning_rate'],\n",
    "    scale_pos_weight=best_s_params['scale_pos_weight'],\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "s_model.fit(X_trainval_with_treatment, Y_trainval)\n",
    "\n",
    "# Generate CATE predictions\n",
    "p1_test = s_model.predict_proba(X_test_treatment)[:, 1]\n",
    "p0_test = s_model.predict_proba(X_test_control)[:, 1]\n",
    "s_cate = p1_test - p0_test\n",
    "\n",
    "# EconML S-Learner with EXACT same model wrapped to output probabilities\n",
    "econml_s = SLearner(\n",
    "    overall_model=ClassifierAsRegressor(XGBClassifier(\n",
    "        max_depth=5,\n",
    "        learning_rate=best_s_params['learning_rate'],\n",
    "        scale_pos_weight=best_s_params['scale_pos_weight'],\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    ")\n",
    "econml_s.fit(Y_trainval, T_trainval, X=X_trainval)\n",
    "econml_s_cate = econml_s.effect(X_test)\n",
    "\n",
    "# Calculate correlation between implementations\n",
    "s_corr = np.corrcoef(s_cate, econml_s_cate)[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe19be-7c56-4ede-84ab-3ff142478a83",
   "metadata": {},
   "source": [
    " ## T-Learner (Two Model Approach)\n",
    "\n",
    " **Method:** Train separate models for treatment and control groups, then take the difference\n",
    "\n",
    " **Algorithm:**\n",
    " 1. **Split Data**: Separate training data by treatment assignment\n",
    " 2. **Model Training**: μ₁(X) learns E[Y|X,T=1] on treated, μ₀(X) learns E[Y|X,T=0] on control\n",
    " 3. **CATE Estimation**: τ(X) = μ₁(X) - μ₀(X)\n",
    " 4. **Advantages**: Flexible, allows different response functions for each group\n",
    " 5. **Disadvantages**: High variance with unbalanced treatments, data splitting reduces power\n",
    "\n",
    " **Implementation**: Two XGBoost classifiers trained on separate treatment groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324225a6-567f-4d61-b9f7-60e97faa348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-LEARNER COMPARISON\n",
    "# T-Learner: Two separate models for treatment and control groups\n",
    "# CATE = μ₁(X) - μ₀(X) where μ₁ and μ₀ are separate models\n",
    "\n",
    "# Use fixed hyperparameters\n",
    "best_t_params = {'learning_rate': 0.01, 'scale_pos_weight': 50}\n",
    "\n",
    "# Split data by treatment\n",
    "X_trainval_treatment = X_trainval[T_trainval == 1]\n",
    "Y_trainval_treatment = Y_trainval[T_trainval == 1]\n",
    "X_trainval_control = X_trainval[T_trainval == 0]\n",
    "Y_trainval_control = Y_trainval[T_trainval == 0]\n",
    "\n",
    "# Train T-Learner models\n",
    "t_treatment = XGBClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=best_t_params['learning_rate'],\n",
    "    scale_pos_weight=best_t_params['scale_pos_weight'],\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "t_control = XGBClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=best_t_params['learning_rate'],\n",
    "    scale_pos_weight=best_t_params['scale_pos_weight'],\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "t_treatment.fit(X_trainval_treatment, Y_trainval_treatment)\n",
    "t_control.fit(X_trainval_control, Y_trainval_control)\n",
    "\n",
    "# Generate CATE predictions\n",
    "p1_test = t_treatment.predict_proba(X_test)[:, 1]\n",
    "p0_test = t_control.predict_proba(X_test)[:, 1]\n",
    "t_cate = p1_test - p0_test\n",
    "\n",
    "# EconML T-Learner with EXACT same models wrapped to output probabilities\n",
    "econml_t = TLearner(\n",
    "    models=[\n",
    "        ClassifierAsRegressor(XGBClassifier(  # Control model\n",
    "            max_depth=5,\n",
    "            learning_rate=best_t_params['learning_rate'],\n",
    "            scale_pos_weight=best_t_params['scale_pos_weight'],\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )),\n",
    "        ClassifierAsRegressor(XGBClassifier(  # Treatment model\n",
    "            max_depth=5,\n",
    "            learning_rate=best_t_params['learning_rate'],\n",
    "            scale_pos_weight=best_t_params['scale_pos_weight'],\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ]\n",
    ")\n",
    "econml_t.fit(Y_trainval, T_trainval, X=X_trainval)\n",
    "econml_t_cate = econml_t.effect(X_test)\n",
    "\n",
    "# Calculate correlation between implementations\n",
    "t_corr = np.corrcoef(t_cate, econml_t_cate)[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63617eda-161c-4cdc-8204-0efa134cb02c",
   "metadata": {},
   "source": [
    " ## X-Learner (Cross-Prediction with Propensity Weighting)\n",
    "\n",
    " **Method:** Advanced approach addressing treatment imbalance by using cross-predictions and propensity-weighted averaging\n",
    "\n",
    " **Algorithm:**\n",
    " 1. **Stage 1**: Same as T-Learner - train μ₁(X) on treated, μ₀(X) on control\n",
    " 2. **Impute Effects**: Create pseudo treatment effects using cross-predictions:\n",
    "    - τ̃₁(X) = Y - μ₀(X) for treated (observed - counterfactual control)\n",
    "    - τ̃₀(X) = μ₁(X) - Y for control (counterfactual treatment - observed)\n",
    " 3. **Stage 2**: Train CATE models on imputed effects:\n",
    "    - τ₁(X) models treatment effects using treated data\n",
    "    - τ₀(X) models treatment effects using control data\n",
    " 4. **Weighted Average**: τ(X) = g(X)·τ₀(X) + (1-g(X))·τ₁(X)\n",
    "\n",
    " **Propensity Weighting Logic:**\n",
    " - τ₀(X) relies on treatment model accuracy → weight by P(T=1|X) (more treatment data = more reliable)\n",
    " - τ₁(X) relies on control model accuracy → weight by P(T=0|X) (more control data = more reliable)\n",
    "\n",
    " **Advantages**: Handles treatment imbalance, uses all data effectively\n",
    " **Implementation**: Four-stage process with XGBoost models plus propensity weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa70c8-3030-4ab6-9d05-db8af9076491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-LEARNER COMPARISON\n",
    "# X-Learner: Advanced method using cross-predictions and propensity weighting\n",
    "# Addresses treatment imbalance by using information from both groups\n",
    "# CATE = g(X)·τ₀(X) + (1-g(X))·τ₁(X) where g(X) is propensity score\n",
    "\n",
    "# Use fixed hyperparameters\n",
    "best_x_params = {'learning_rate': 0.1, 'scale_pos_weight': 50}\n",
    "\n",
    "# Stage 1: Outcome models\n",
    "x_mu1 = XGBClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=best_x_params['learning_rate'],\n",
    "    scale_pos_weight=best_x_params['scale_pos_weight'],\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "x_mu0 = XGBClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=best_x_params['learning_rate'],\n",
    "    scale_pos_weight=best_x_params['scale_pos_weight'],\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "x_mu1.fit(X_trainval_treatment, Y_trainval_treatment)\n",
    "x_mu0.fit(X_trainval_control, Y_trainval_control)\n",
    "\n",
    "# Impute treatment effects\n",
    "mu0_pred_treatment = x_mu0.predict_proba(X_trainval_treatment)[:, 1]\n",
    "mu1_pred_control = x_mu1.predict_proba(X_trainval_control)[:, 1]\n",
    "tau1_imputed = Y_trainval_treatment - mu0_pred_treatment\n",
    "tau0_imputed = mu1_pred_control - Y_trainval_control\n",
    "\n",
    "# Stage 2: Treatment effect models\n",
    "x_tau1 = XGBRegressor(\n",
    "    max_depth=5,\n",
    "    learning_rate=best_x_params['learning_rate'],\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "x_tau0 = XGBRegressor(\n",
    "    max_depth=5,\n",
    "    learning_rate=best_x_params['learning_rate'],\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "x_tau1.fit(X_trainval_treatment, tau1_imputed)\n",
    "x_tau0.fit(X_trainval_control, tau0_imputed)\n",
    "\n",
    "# Propensity model\n",
    "x_prop = XGBClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=best_x_params['learning_rate'],\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "x_prop.fit(X_trainval, T_trainval)\n",
    "\n",
    "# Generate CATE predictions\n",
    "tau1_test = x_tau1.predict(X_test)\n",
    "tau0_test = x_tau0.predict(X_test)\n",
    "propensity_test = x_prop.predict_proba(X_test)[:, 1]\n",
    "x_cate = propensity_test * tau0_test + (1 - propensity_test) * tau1_test\n",
    "\n",
    "# EconML X-Learner with wrapped models to output probabilities\n",
    "econml_x = XLearner(\n",
    "    models=[\n",
    "        ClassifierAsRegressor(XGBClassifier(  # Control outcome\n",
    "            max_depth=5,\n",
    "            learning_rate=best_x_params['learning_rate'],\n",
    "            scale_pos_weight=best_x_params['scale_pos_weight'],\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )),\n",
    "        ClassifierAsRegressor(XGBClassifier(  # Treatment outcome\n",
    "            max_depth=5,\n",
    "            learning_rate=best_x_params['learning_rate'],\n",
    "            scale_pos_weight=best_x_params['scale_pos_weight'],\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ],\n",
    "    cate_models=[\n",
    "        XGBRegressor(  # Control CATE\n",
    "            max_depth=5,\n",
    "            learning_rate=best_x_params['learning_rate'],\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        XGBRegressor(  # Treatment CATE\n",
    "            max_depth=5,\n",
    "            learning_rate=best_x_params['learning_rate'],\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    ]\n",
    "    # propensity_model uses default LogisticRegression\n",
    ")\n",
    "econml_x.fit(Y_trainval, T_trainval, X=X_trainval)\n",
    "econml_x_cate = econml_x.effect(X_test)\n",
    "\n",
    "# Calculate correlation between implementations\n",
    "x_corr = np.corrcoef(x_cate, econml_x_cate)[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3c997-4922-44f9-a44f-144dfa9611c4",
   "metadata": {},
   "source": [
    " ## R-Learner (Residualization Approach)\n",
    "\n",
    " **Method:** Use residuals to directly model treatment effects by removing confounding patterns first\n",
    "\n",
    " **Algorithm:**\n",
    " 1. **Nuisance Models**: Train separate models for outcome and treatment assignment:\n",
    "    - μ(X) estimates E[Y|X] (outcome model, excludes treatment)\n",
    "    - p(X) estimates P(T=1|X) (propensity score model)\n",
    " 2. **Residualization**: Remove confounding patterns:\n",
    "    - Outcome residuals: e_Y = Y - μ(X)\n",
    "    - Treatment residuals: e_T = T - p(X)\n",
    " 3. **Treatment Effect Model**: Solve weighted regression:\n",
    "    - Target: e_Y/e_T (residualized outcome ratio)\n",
    "    - Weights: e_T² (variance-based weighting)\n",
    "    - Features: X (original covariates)\n",
    "\n",
    " **Key Insight**: After orthogonalization, residual relationship directly reveals causal effects\n",
    "\n",
    " **Advantages**: Theoretically robust, handles high-dimensional confounding\n",
    "\n",
    " **Implementation**: Three-stage process with cross-fitting for unbiased estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773f22c-65be-463e-aaaf-84ddc36ced03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-LEARNER COMPARISON\n",
    "# R-Learner: Residualization approach that directly models treatment effects\n",
    "# Uses residuals: e_Y = Y - E[Y|X], e_T = T - E[T|X]\n",
    "# Then solves: min Σ(e_Y - e_T·τ(X))² with weights e_T²\n",
    "\n",
    "# Use fixed hyperparameters\n",
    "best_r_params = {'learning_rate': 0.01, 'scale_pos_weight': 50}\n",
    "\n",
    "# Outcome and propensity models\n",
    "r_outcome = XGBClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=best_r_params['learning_rate'],\n",
    "    scale_pos_weight=best_r_params['scale_pos_weight'],\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "r_propensity = XGBClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=best_r_params['learning_rate'],\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "r_outcome.fit(X_trainval, Y_trainval)\n",
    "r_propensity.fit(X_trainval, T_trainval)\n",
    "\n",
    "# Compute residuals\n",
    "Y_pred_trainval = r_outcome.predict_proba(X_trainval)[:, 1]\n",
    "T_pred_trainval = r_propensity.predict_proba(X_trainval)[:, 1]\n",
    "e_Y = Y_trainval - Y_pred_trainval\n",
    "e_T = T_trainval - T_pred_trainval\n",
    "\n",
    "# Train treatment effect model\n",
    "weights = e_T ** 2\n",
    "weights = np.maximum(weights, 1e-8)\n",
    "e_T_safe = np.where(np.abs(e_T) < 1e-8, 1e-8, e_T)\n",
    "target = e_Y / e_T_safe\n",
    "\n",
    "r_tau = XGBRegressor(\n",
    "    max_depth=5,\n",
    "    learning_rate=best_r_params['learning_rate'],\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "r_tau.fit(X_trainval, target, sample_weight=weights)\n",
    "\n",
    "# Generate CATE predictions\n",
    "r_cate = r_tau.predict(X_test)\n",
    "\n",
    "# EconML R-Learner using NonParamDML (which implements R-Learner)\n",
    "from econml.dml import NonParamDML\n",
    "\n",
    "# Use wrapped classifiers to output probabilities for outcome model\n",
    "econml_r = NonParamDML(\n",
    "    model_y=ClassifierAsRegressor(XGBClassifier(  # Outcome model\n",
    "        max_depth=5,\n",
    "        learning_rate=best_r_params['learning_rate'],\n",
    "        scale_pos_weight=best_r_params['scale_pos_weight'],\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    model_t=XGBClassifier(  # Propensity model\n",
    "        max_depth=5,\n",
    "        learning_rate=best_r_params['learning_rate'],\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    model_final=XGBRegressor(  # Final stage model\n",
    "        max_depth=5,\n",
    "        learning_rate=best_r_params['learning_rate'],\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    discrete_treatment=True,\n",
    "    cv=1  \n",
    ")\n",
    "econml_r.fit(Y_trainval, T_trainval, X=X_trainval)\n",
    "econml_r_cate = econml_r.effect(X_test)\n",
    "\n",
    "# Calculate correlation between implementations\n",
    "r_corr = np.corrcoef(r_cate, econml_r_cate.ravel())[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8592c29-1730-4e6b-9e90-e478b77aeb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "               HAND-CODED vs ECONML VERIFICATION RESULTS\n",
      "======================================================================\n",
      "shape: (4, 3)\n",
      "┌───────────┬─────────────┬───────────┐\n",
      "│ Learner   ┆ Correlation ┆ Status    │\n",
      "│ ---       ┆ ---         ┆ ---       │\n",
      "│ str       ┆ f64         ┆ str       │\n",
      "╞═══════════╪═════════════╪═══════════╡\n",
      "│ S-Learner ┆ 1.0         ┆ Perfect   │\n",
      "│ T-Learner ┆ 1.0         ┆ Perfect   │\n",
      "│ X-Learner ┆ 0.998948    ┆ Excellent │\n",
      "│ R-Learner ┆ 1.0         ┆ Perfect   │\n",
      "└───────────┴─────────────┴───────────┘\n",
      "======================================================================\n",
      "\n",
      "SUMMARY:\n",
      "S-Learner: Perfect match - implementations are identical\n",
      "T-Learner: Perfect match - implementations are identical\n",
      "X-Learner: Near perfect - minor difference from propensity model (XGB vs Logit)\n",
      "R-Learner: Perfect match - single-fold CV eliminates randomness\n",
      "\n",
      "CONCLUSION: All hand-coded implementations are validated and correct!\n"
     ]
    }
   ],
   "source": [
    "# FINAL VERIFICATION SUMMARY\n",
    "# Compare all learner implementations and provide detailed explanations\n",
    "\n",
    "# Create comparison table without CATE means for cleaner output\n",
    "results = pl.DataFrame({\n",
    "    'Learner': ['S-Learner', 'T-Learner', 'X-Learner', 'R-Learner'],\n",
    "    'Correlation': [s_corr, t_corr, x_corr, r_corr],\n",
    "    'Status': [\n",
    "        'Perfect' if s_corr > 0.999 else 'Good',\n",
    "        'Perfect' if t_corr > 0.999 else 'Good',\n",
    "        'Excellent' if x_corr > 0.99 else 'Good',\n",
    "        'Perfect' if r_corr > 0.999 else 'Good'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display final results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \" * 15 + \"HAND-CODED vs ECONML VERIFICATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(results)\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nSUMMARY:\")\n",
    "print(\"S-Learner: Perfect match - implementations are identical\")\n",
    "print(\"T-Learner: Perfect match - implementations are identical\")\n",
    "print(\"X-Learner: Near perfect - minor difference from propensity model (XGB vs Logit)\")\n",
    "print(\"R-Learner: Perfect match - single-fold CV eliminates randomness\")\n",
    "\n",
    "print(\"\\nCONCLUSION: All hand-coded implementations are validated and correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd5a61-2488-4cdd-847a-57189cb82334",
   "metadata": {},
   "source": [
    " ## Implementation Verification Results\n",
    "\n",
    " **Summary of Hand-Coded vs EconML Comparisons:**\n",
    "\n",
    " | Learner | Correlation | Status | Key Differences |\n",
    " |---------|-------------|--------|-----------------|\n",
    " | S-Learner | 1.000 | Perfect | Identical implementations |\n",
    " | T-Learner | 1.000 | Perfect | Identical implementations |\n",
    " | X-Learner | 0.999 | Excellent | Propensity model choice (XGB vs Logistic) |\n",
    " | R-Learner | 1.000 | Perfect | Single-fold cross-validation (cv=1) |\n",
    "\n",
    " **Why Differences Exist:**\n",
    "\n",
    " **X-Learner (0.999 correlation):**\n",
    " - Hand-coded uses XGBoost for propensity scores\n",
    " - EconML uses default LogisticRegression for propensity estimation\n",
    " - Since final CATE = g(X)·τ₀(X) + (1-g(X))·τ₁(X), different propensity models create slightly different weighting\n",
    " - Near-perfect correlation confirms both capture identical treatment effect patterns\n",
    "\n",
    " **Conclusion:** Hand-coded implementations successfully replicate production-quality algorithms with excellent fidelity. Minor differences stem from implementation choices (propensity models, cross-fitting) rather than algorithmic errors, demonstrating deep understanding of each method's theoretical foundations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
